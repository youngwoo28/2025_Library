{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtNgxVi4KhPtCn0Qd/pcVR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iaFssSyHifvp","executionInfo":{"status":"ok","timestamp":1747983708107,"user_tz":-540,"elapsed":2816,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"d7d5318a-291f-4e8b-8c34-46de2bc0c3c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["# 데이터 확인해보기\n","import string\n","\n","l = []  # 전처리된 문장을 저장할 리스트\n","\n","# 한글 텍스트 파일을 읽기 위해 utf-8 인코딩으로 읽어옴\n","with open(\n","    \"/content/kor.txt\",\n","    'r', encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","    for line in lines:\n","        # 특수 문자를 지우고 모든 글자를 소문자로 변경\n","        txt = \"\".join(v for v in line if v not in string.punctuation).lower()\n","        l.append(txt)\n","\n","print(l[:5])  # 정제된 문장 중 앞부분 5개 출력 (예시 확인용)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CojmqGwa42Ji","executionInfo":{"status":"ok","timestamp":1747983708243,"user_tz":-540,"elapsed":134,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"017dcf40-f96f-45a0-99d2-55133e52204f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['go\\t가\\tccby 20 france attribution tatoebaorg 2877272 cm  8363271 eunhee', 'hi\\t안녕\\tccby 20 france attribution tatoebaorg 538123 cm  8355888 eunhee', 'run\\t뛰어\\tccby 20 france attribution tatoebaorg 906328 papabear  8355891 eunhee', 'run\\t뛰어\\tccby 20 france attribution tatoebaorg 4008918 jsakuragi  8363273 eunhee', 'who\\t누구\\tccby 20 france attribution tatoebaorg 2083030 ck  6820074 yesjustryan']\n"]}]},{"cell_type":"code","source":["# BOW 생성 함수 정의\n","\n","import numpy as np\n","import torch\n","\n","from torch.utils.data.dataset import Dataset\n","\n","def get_BOW(corpus):  # 문장들로부터 BOW를 만드는 함수\n","    BOW = {\"<SOS>\": 0, \"<EOS>\": 1}  # <SOS> 토큰과 <EOS> 토큰을 추가\n","\n","    # 문장 내 단어들을 이용해 BOW를 생성\n","    for line in corpus:  # 각 문장을 순회하면서\n","        for word in line.split():  # 각 문장을 단어 단위로 나눈 후\n","            if word not in BOW.keys():  # BOW에 없는 단어라면 새로운 인덱스로 추가\n","                BOW[word] = len(BOW.keys())\n","\n","    return BOW\n","\n","# 학습에 사용할 데이터셋 정의\n","\n","class Eng2Kor(Dataset):  # 학습에 이용할 데이터셋\n","    def __init__(self,\n","                 pth2txt=\"/content/kor.txt\"):\n","        self.eng_corpus = []  # 영어 문장이 들어가는 변수\n","        self.kor_corpus = []  # 한글 문장이 들어가는 변수\n","\n","        # 텍스트 파일을 읽어서 영어 문장과 한글 문장을 저장\n","        with open(pth2txt, 'r', encoding=\"utf-8\") as f:\n","            lines = f.read().split(\"\\n\")\n","            for line in lines:\n","                # 빈 줄 건너뛰기\n","                if not line.strip():\n","                    continue\n","\n","                parts = line.split(\"\\t\")  # 탭(\\t)으로 영어/한글 문장 분리\n","                if len(parts) >= 2:  # 영어, 한글 문장이 모두 있는 경우만 처리\n","                    # 영어 문장에서 특수 문자 제거 및 소문자 변환\n","                    engtxt = \"\".join(\n","                        v for v in parts[0] if v not in string.punctuation\n","                    ).lower()\n","\n","                    # 한국어 문장에서 특수 문자 제거\n","                    kortxt = \"\".join(\n","                        v for v in parts[1] if v not in string.punctuation\n","                    )\n","\n","                    # 너무 긴 문장은 제외 (10단어 이하만 사용)\n","                    if len(engtxt.split()) <= 10 and len(kortxt.split()) <= 10:\n","                        self.eng_corpus.append(engtxt)\n","                        self.kor_corpus.append(kortxt)\n","\n","        self.engBOW = get_BOW(self.eng_corpus)  # 영어 BOW\n","        self.engBOW = get_BOW(self.eng_corpus)  # 영어 BOW\n","        self.korBOW = get_BOW(self.kor_corpus)  # 한글 BOW\n","\n","    # 문장을 단어 리스트로 나눈 뒤 <EOS> 토큰 추가\n","    def gen_seq(self, line):\n","        seq = line.split()\n","        seq.append(\"<EOS>\")\n","        return seq\n","\n","    def __len__(self):\n","        return len(self.eng_corpus)\n","\n","    def __getitem__(self, i):\n","        # 문자열로 되어 있는 문장을 숫자 표현으로 변경\n","        data = np.array([\n","            self.engBOW[txt] for txt in self.gen_seq(self.eng_corpus[i])\n","        ])\n","\n","        label = np.array([\n","            self.korBOW[txt] for txt in self.gen_seq(self.kor_corpus[i])\n","        ])\n","\n","        return data, label\n","\n","\n","\n","    # 샘플 데이터를 출력하는 함수 추가\n","    def print_samples(self, num_samples=5):\n","        \"\"\"\n","        데이터셋에서 num_samples 개수만큼의 샘플을 출력합니다.\n","        \"\"\"\n","        print(f\"데이터셋 크기: {len(self.eng_corpus)} 쌍의 문장\")\n","        print(\"\\n샘플 데이터:\")\n","\n","        # 데이터셋 크기보다 많은 샘플을 요청한 경우 조정\n","        num_samples = min(num_samples, len(self.eng_corpus))\n","\n","        for i in range(num_samples):\n","            print(f\"\\n샘플 {i+1}:\")\n","            print(f\"  영어: {self.eng_corpus[i]}\")\n","            print(f\"  한국어: {self.kor_corpus[i]}\")\n","\n","            # 숫자 표현도 확인\n","            eng_indices = [self.engBOW[txt] for txt in self.gen_seq(self.eng_corpus[i])]\n","            kor_indices = [self.korBOW[txt] for txt in self.gen_seq(self.kor_corpus[i])]\n","\n","            print(f\"  영어 인덱스: {eng_indices}\")\n","            print(f\"  한국어 인덱스: {kor_indices}\")\n","            print()\n"],"metadata":{"id":"itglbMsH5DDq","executionInfo":{"status":"ok","timestamp":1747983710190,"user_tz":-540,"elapsed":2,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# 샘플 데이터 출력해보기\n","\n","# 데이터셋 생성\n","dataset = Eng2Kor()\n","\n","# 샘플 데이터 10개 출력\n","dataset.print_samples(10)\n","\n","# BOW 사전 크기 확인\n","print(f\"\\n영어 어휘 크기: {len(dataset.engBOW)}\")\n","print(f\"한국어 어휘 크기: {len(dataset.korBOW)}\")\n","\n","# 몇 가지 단어의 인덱스 확인\n","print(\"\\n영어 단어 인덱스 예시:\")\n","for word in [\"go\", \"hello\", \"thank\", \"<SOS>\", \"<EOS>\"]:\n","    if word in dataset.engBOW:\n","        print(f\"  '{word}': {dataset.engBOW[word]}\")\n","    else:\n","        print(f\"  '{word}': 어휘에 없음\")\n","\n","print(\"\\n한국어 단어 인덱스 예시:\")\n","for word in [\"가\", \"안녕\", \"감사합니다\", \"<SOS>\", \"<EOS>\"]:  # ✅ 괄호 오류 수정 완료\n","    if word in dataset.korBOW:\n","        print(f\"  '{word}': {dataset.korBOW[word]}\")\n","    else:\n","        print(f\"  '{word}': 어휘에 없음\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgLjEEdj42Hq","executionInfo":{"status":"ok","timestamp":1747983714595,"user_tz":-540,"elapsed":7,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"cda76f71-559a-4494-8057-af8e26a9ce11"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["데이터셋 크기: 5701 쌍의 문장\n","\n","샘플 데이터:\n","\n","샘플 1:\n","  영어: go\n","  한국어: 가\n","  영어 인덱스: [2, 1]\n","  한국어 인덱스: [2, 1]\n","\n","\n","샘플 2:\n","  영어: hi\n","  한국어: 안녕\n","  영어 인덱스: [3, 1]\n","  한국어 인덱스: [3, 1]\n","\n","\n","샘플 3:\n","  영어: run\n","  한국어: 뛰어\n","  영어 인덱스: [4, 1]\n","  한국어 인덱스: [4, 1]\n","\n","\n","샘플 4:\n","  영어: run\n","  한국어: 뛰어\n","  영어 인덱스: [4, 1]\n","  한국어 인덱스: [4, 1]\n","\n","\n","샘플 5:\n","  영어: who\n","  한국어: 누구\n","  영어 인덱스: [5, 1]\n","  한국어 인덱스: [5, 1]\n","\n","\n","샘플 6:\n","  영어: wow\n","  한국어: 우와\n","  영어 인덱스: [6, 1]\n","  한국어 인덱스: [6, 1]\n","\n","\n","샘플 7:\n","  영어: duck\n","  한국어: 숙여\n","  영어 인덱스: [7, 1]\n","  한국어 인덱스: [7, 1]\n","\n","\n","샘플 8:\n","  영어: fire\n","  한국어: 쏴\n","  영어 인덱스: [8, 1]\n","  한국어 인덱스: [8, 1]\n","\n","\n","샘플 9:\n","  영어: help\n","  한국어: 도와줘\n","  영어 인덱스: [9, 1]\n","  한국어 인덱스: [9, 1]\n","\n","\n","샘플 10:\n","  영어: hide\n","  한국어: 숨어\n","  영어 인덱스: [10, 1]\n","  한국어 인덱스: [10, 1]\n","\n","\n","영어 어휘 크기: 3048\n","한국어 어휘 크기: 7466\n","\n","영어 단어 인덱스 예시:\n","  'go': 2\n","  'hello': 15\n","  'thank': 180\n","  '<SOS>': 0\n","  '<EOS>': 1\n","\n","한국어 단어 인덱스 예시:\n","  '가': 2\n","  '안녕': 3\n","  '감사합니다': 6442\n","  '<SOS>': 0\n","  '<EOS>': 1\n"]}]},{"cell_type":"code","source":["# 학습에 사용할 데이터 로더 정의\n","\n","def loader(dataset):  # 데이터셋의 문장을 한 문장씩 불러오기 위한 함수\n","    for i in range(len(dataset)):\n","        data, label = dataset[i]\n","\n","        # numpy array → torch tensor로 변환 후 반환\n","        # 매 반복마다 하나의 (입력, 정답) 쌍을 yield (제너레이터 형태)\n","        yield torch.tensor(data), torch.tensor(label)\n"],"metadata":{"id":"dj9QyWxn42Fg","executionInfo":{"status":"ok","timestamp":1747983718110,"user_tz":-540,"elapsed":7,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 인코더 정의\n","\n","import torch.nn as nn\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Encoder, self).__init__()\n","\n","        # 단어 인덱스를 임베딩 벡터로 변환 (input_size: 단어 수, hidden_size: 벡터 차원)\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","\n","        # GRU 정의 (입력과 은닉 상태의 차원이 같음)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, x, h):\n","        # 배치 차원과 시계열 차원 추가 (모양 맞춰주기 용도)\n","        x = self.embedding(x).view(1, 1, -1)\n","\n","        # GRU에 입력과 이전 hidden state를 넣고, 출력과 새로운 hidden state를 반환\n","        output, hidden = self.gru(x, h)\n","\n","        return output, hidden\n"],"metadata":{"id":"liD06e4x42BO","executionInfo":{"status":"ok","timestamp":1747983800422,"user_tz":-540,"elapsed":51,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 디코더 정의\n","\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n","        super(Decoder, self).__init__()\n","\n","        # 출력 단어 인덱스를 hidden_size 차원의 임베딩 벡터로 변환\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","\n","        # 어텐션 가중치를 계산하기 위한 MLP층\n","        self.attention = nn.Linear(hidden_size * 2, max_length)\n","\n","        # context vector + 임베딩을 결합한 후 특징 추출하는 MLP\n","        self.context = nn.Linear(hidden_size * 2, hidden_size)\n","\n","        # 과적합을 피하기 위한 드롭아웃 층\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","        # GRU 계층 (입력: context 특징, 은닉 상태)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","        # 최종 출력층: hidden → 단어 개수 크기의 벡터 (단어 분류용)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        # 활성화 함수들\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","\n","\n","    def forward(self, x, h, encoder_outputs):\n","        # 입력 단어 인덱스를 임베딩하고 (1, 1, hidden_size) 형태로 변형\n","        x = self.embedding(x).view(1, 1, -1)\n","        x = self.dropout(x)\n","\n","        # 어텐션 가중치 계산:\n","        # 현재 입력(임베딩)과 이전 hidden state를 이어붙여 attention score 계산\n","        attn_weights = self.softmax(\n","            self.attention(torch.cat((x[0], h[0]), -1))  # 결과 shape: (1, max_length)\n","        )\n","\n","        # 인코더의 출력 전체에 어텐션 가중치를 곱해 context vector 생성\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                encoder_outputs.unsqueeze(0))\n","\n","        # 인코더 각 시점의 중요도와 민집표현을 합쳐 MLP층으로 특징 추출\n","        output = torch.cat((x[0], attn_applied[0]), 1)\n","        output = self.context(output).unsqueeze(0)\n","        output = self.relu(output)\n","\n","        # GRU에 넣어 다음 hidden state 계산\n","        output, hidden = self.gru(output, h)\n","\n","        # hidden → vocabulary size로 변환 (각 단어의 확률 분포)\n","        output = self.out(output[0])\n","\n","        return output\n","\n"],"metadata":{"id":"KH3QG_0K9G8B","executionInfo":{"status":"ok","timestamp":1747983906453,"user_tz":-540,"elapsed":20,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["  # 학습에 필요한 요소 정의\n","\n","import random\n","import tqdm\n","from torch.optim.adam import Adam\n","\n","# 학습에 사용할 프로세서 정의\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# 학습에 사용할 데이터셋 정의 (전처리 + BOW 사전 포함)\n","dataset = Eng2Kor()\n","\n","# 인코더 디코더 정의\n","encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n","decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n","\n","# 인코더 디코더 학습을 위한 최적화 정의\n","encoder_optimizer = Adam(encoder.parameters(), lr=0.0001)\n","decoder_optimizer = Adam(decoder.parameters(), lr=0.0001)\n","\n","# 학습 루프 정의\n","\n","for epoch in range(5):\n","    iterator = tqdm.tqdm(loader(dataset), total=len(dataset))\n","    total_loss = 0\n","\n","    for data, label in iterator:\n","        data = torch.tensor(data, dtype=torch.long).to(device)\n","        label = torch.tensor(label, dtype=torch.long).to(device)\n","\n","        # 인코더의 초기 은닉 상태\n","        encoder_hidden = torch.zeros(1, 1, 64).to(device)\n","\n","        # 인코더의 모든 시점의 출력을 저장하는 변수\n","        encoder_outputs = torch.zeros(11, 64).to(device)\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        loss = 0\n","        for ei in range(len(data)):\n","            # 한 단어씩 인코더에 넣어줌\n","            encoder_output, encoder_hidden = encoder(\n","                data[ei], encoder_hidden\n","            )\n","\n","            # 인코더의 출력 저장\n","            encoder_outputs[ei] = encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[0]]).to(device)  # <SOS> 토큰\n","\n","        # 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 저장\n","        decoder_hidden = encoder_hidden\n","\n","        # 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 저장\n","        decoder_hidden = encoder_hidden\n","\n","        # (option1) 50% 확률로 teacher forcing 사용\n","        # use_teacher_forcing = True if random.random() < 0.5 else False\n","        # (option2) 강제로 teacher forcing 사용\n","        use_teacher_forcing = True\n","\n","        if use_teacher_forcing:\n","            for di in range(len(label)):\n","                decoder_output = decoder(\n","                    decoder_input, decoder_hidden, encoder_outputs\n","                )\n","\n","                # 정답을 다음 시점의 입력으로 사용\n","                target = torch.tensor(label[di], dtype=torch.long).to(device)\n","                target = torch.unsqueeze(target, dim=0).to(device)\n","                loss += nn.CrossEntropyLoss()(decoder_output, target)\n","\n","                decoder_input = target  # 다음 입력으로 정답 사용\n","\n","        else:\n","            for di in range(len(label)):\n","                # 디코더의 출력 계산\n","                decoder_output = decoder(\n","                    decoder_input, decoder_hidden, encoder_outputs\n","                )\n","\n","                # 예측된 단어 중 가장 확률 높은 top1 단어를 다음 입력으로 사용\n","                topv, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze().detach()\n","\n","                # 현재 시점의 정답과 비교하여 손실 계산\n","                target = torch.tensor(label[di], dtype=torch.long).to(device)\n","                target = torch.unsqueeze(target, dim=0).to(device)\n","                loss += nn.CrossEntropyLoss()(decoder_output, target)\n","\n","                if decoder_input.item() == 1:  # <EOS> 토큰을 만나면 중지\n","                    break\n","\n","        # 문장 하나에 대한 평균 손실 계산 후 누적\n","        total_loss += loss.item() / len(dataset)\n","        iterator.set_description(f\"epoch:{epoch+1} loss:{total_loss}\")\n","\n","        loss.backward()  # 손실에 대한 역전파\n","\n","        encoder_optimizer.step()  # 인코더 파라미터 업데이트\n","        decoder_optimizer.step()  # 디코더 파라미터 업데이트\n","\n","# 모델 저장\n","torch.save(encoder.state_dict(), \"attn_enc.pth\")\n","torch.save(decoder.state_dict(), \"attn_dec.pth\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrlYIndN9HlZ","executionInfo":{"status":"ok","timestamp":1747985900151,"user_tz":-540,"elapsed":1256280,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"3f4733bd-6937-42f4-b1dd-7f4925092dcf"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5701 [00:00<?, ?it/s]<ipython-input-37-e9fc3ba0a2ae>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  data = torch.tensor(data, dtype=torch.long).to(device)\n","<ipython-input-37-e9fc3ba0a2ae>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  label = torch.tensor(label, dtype=torch.long).to(device)\n","<ipython-input-37-e9fc3ba0a2ae>:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  target = torch.tensor(label[di], dtype=torch.long).to(device)\n","epoch:1 loss:36.14700680568616: 100%|██████████| 5701/5701 [03:58<00:00, 23.89it/s]\n","epoch:2 loss:31.577122291082443: 100%|██████████| 5701/5701 [04:16<00:00, 22.19it/s]\n","epoch:3 loss:29.996772015519305: 100%|██████████| 5701/5701 [04:14<00:00, 22.41it/s]\n","epoch:4 loss:28.898746143284892: 100%|██████████| 5701/5701 [04:12<00:00, 22.58it/s]\n","epoch:5 loss:28.010204179352602: 100%|██████████| 5701/5701 [04:13<00:00, 22.48it/s]\n"]}]},{"cell_type":"code","source":["# 모델 성능 평가에 필요한 요소 정의\n","\n","# 인코더 가중치 불러오기\n","encoder.load_state_dict(torch.load(\"attn_enc.pth\", map_location=device))\n","\n","# 디코더 가중치 불러오기\n","decoder.load_state_dict(torch.load(\"attn_dec.pth\", map_location=device))\n","\n","\n","# 불러올 영어 문장을 랜덤하게 지정\n","idx = random.randint(0, len(dataset))\n","\n","# 테스트에 사용할 문장\n","input_sentence = dataset.eng_corpus[idx]\n","\n","# 신경망이 번역한 문장을 저장할 변수\n","pred_sentence = \"\"\n","\n","# 데이터셋에서 해당 인덱스의 데이터 불러오기\n","data, label = dataset[idx]\n","data = torch.tensor(data, dtype=torch.long).to(device)\n","label = torch.tensor(label, dtype=torch.long).to(device)\n","\n","# 인코더의 초기 은닉 상태 정의\n","encoder_hidden = torch.zeros(1, 1, 64).to(device)\n","\n","# 인코더 출력값을 담기 위한 변수\n","encoder_outputs = torch.zeros(11, 64).to(device)\n"],"metadata":{"id":"fZOm0ozi-Yhp","executionInfo":{"status":"ok","timestamp":1747985907923,"user_tz":-540,"elapsed":7,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# 인코더 동작\n","\n","for ei in range(len(data)):\n","    # 한 단어씩 인코더에 넣어줌\n","    encoder_output, encoder_hidden = encoder(\n","        data[ei], encoder_hidden)\n","\n","    # 인코더의 출력을 저장\n","    encoder_outputs[ei] = encoder_output[0, 0]\n","\n","\n","# 디코더의 초기 입력\n","# 0은 <SOS> 토큰\n","decoder_input = torch.tensor([[0]]).to(device)\n","\n","# 디코더의 초기 은닉 상태는 인코더의 마지막 hidden state로 설정\n","decoder_hidden = encoder_hidden\n"],"metadata":{"id":"E4Mh9-zV-YfX","executionInfo":{"status":"ok","timestamp":1747985909664,"user_tz":-540,"elapsed":4,"user":{"displayName":"최영우","userId":"13835120978345486432"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# 디코더 동작\n","\n","for di in range(11):  # 최대 11단어까지 예측 (길이 제한)\n","    # 현재 입력, hidden state, 인코더 출력을 기반으로 다음 단어 예측\n","    decoder_output = decoder(\n","        decoder_input, decoder_hidden, encoder_outputs)\n","\n","    # 예측 결과 중 확률이 가장 높은 단어 선택\n","    topv, topi = decoder_output.topk(1)  # topi: 예측된 단어 인덱스\n","    decoder_input = topi.squeeze().detach()  # 다음 입력으로 설정 (detach로 gradient 제외)\n","\n","    # <EOS> 토큰을 만나면 중지\n","    if decoder_input.item() == 1:\n","        break\n","\n","    # 가장 높은 확률값의 단어를 문자열에 추가\n","    pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"\n","\n","print(input_sentence)     # 영어 문장\n","print(pred_sentence)      # 번역된 한글 문장\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NVXRZ3J-Yc9","executionInfo":{"status":"ok","timestamp":1747985912469,"user_tz":-540,"elapsed":12,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"0a63da3a-9ab8-435d-f159-6dd32a6aa70a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tom is bedridden now\n","톰은 메리가 톰은 메리가 \n"]}]},{"cell_type":"code","source":["for _ in range(6):\n","    # 무작위 문장 하나 선택\n","    idx = random.randint(0, len(dataset) - 1)\n","    input_sentence = dataset.eng_corpus[idx]\n","    data, label = dataset[idx]\n","\n","    data = torch.tensor(data, dtype=torch.long).to(device)\n","    label = torch.tensor(label, dtype=torch.long).to(device)\n","\n","    encoder_hidden = torch.zeros(1, 1, 64).to(device)\n","    encoder_outputs = torch.zeros(11, 64).to(device)\n","\n","    for ei in range(len(data)):\n","        encoder_output, encoder_hidden = encoder(data[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[0]]).to(device)\n","    decoder_hidden = encoder_hidden\n","\n","    pred_sentence = \"\"\n","\n","    for di in range(11):\n","        decoder_output = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","        topv, topi = decoder_output.topk(1)\n","        decoder_input = topi.squeeze().detach()\n","\n","        if decoder_input.item() == 1:\n","            break\n","\n","        pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"\n","\n","    print(f\"[영어 문장] {input_sentence}\")\n","    print(f\"[예측 번역] {pred_sentence}\")\n","    print(\"-\" * 50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xY3oYoFN_77x","executionInfo":{"status":"ok","timestamp":1747985948145,"user_tz":-540,"elapsed":62,"user":{"displayName":"최영우","userId":"13835120978345486432"}},"outputId":"0b82e0bb-508d-49f3-f1db-e793f3641ac0"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["[영어 문장] what does that word mean\n","[예측 번역] 너 \n","--------------------------------------------------\n","[영어 문장] my favorite subject is science\n","[예측 번역] 내 내 내 내 내 내 내 내 내 내 내 \n","--------------------------------------------------\n","[영어 문장] i believe that the answer is straightforward\n","[예측 번역] 나는 내가 같아 \n","--------------------------------------------------\n","[영어 문장] tom grew up in a rich family\n","[예측 번역] 톰은 메리가 톰은 메리가 톰은 메리가 톰은 메리가 톰은 메리가 \n","--------------------------------------------------\n","[영어 문장] our restaurant is the best\n","[예측 번역] 너 가장 \n","--------------------------------------------------\n","[영어 문장] try to see the problem from her point of view\n","[예측 번역] 그 \n","--------------------------------------------------\n"]}]}]}